{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training main model...\n",
      "Starting training for sentence-level prediction...\n",
      "Found 50 video files and 50 alignment files\n",
      "Processing alignment files from: /Users/sureshkumar/Desktop/Mouthmap/datas/alignments/s1\n",
      "Vocabulary size: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 22:01:31.135273: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-01-23 22:01:31.135338: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-01-23 22:01:31.135343: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "2025-01-23 22:01:31.135371: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-01-23 22:01:31.135401: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-23 22:01:36.819374: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 174s/step - accuracy: 0.0000e+00 - loss: 20.1747\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 238s/step - accuracy: 0.1250 - loss: 20.1745\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 187s/step - accuracy: 0.1250 - loss: 20.1744\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 174s/step - accuracy: 0.1250 - loss: 20.1742\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 158s/step - accuracy: 0.1250 - loss: 20.1741\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 160s/step - accuracy: 0.1250 - loss: 20.1739\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 239s/step - accuracy: 0.1250 - loss: 20.1738\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 146s/step - accuracy: 0.1250 - loss: 20.1736\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 185s/step - accuracy: 0.1250 - loss: 20.1735\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 148s/step - accuracy: 0.1250 - loss: 20.1733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model saved: models_main/lip_reading_main_final.h5\n",
      "Vocabulary saved: models_main/vocabulary_main.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv3D, LSTM, Dense, Dropout, Bidirectional, MaxPool3D, BatchNormalization, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from glob import glob\n",
    "import cv2\n",
    "\n",
    "class LipReadingDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_path, alignment_path, batch_size=32, frame_length=75, \n",
    "                 image_height=46, image_width=140, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.data_path = data_path\n",
    "        self.alignment_path = alignment_path\n",
    "        self.batch_size = batch_size\n",
    "        self.frame_length = frame_length\n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "\n",
    "        self.video_paths = sorted(glob(os.path.join(data_path, '*.mpg')))\n",
    "        self.alignment_paths = sorted(glob(os.path.join(alignment_path, '*.align')))\n",
    "        \n",
    "        print(f\"Found {len(self.video_paths)} video files and {len(self.alignment_paths)} alignment files\")\n",
    "        self.vocabulary = self._create_word_vocabulary()\n",
    "            \n",
    "        self.char_to_num = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vocabulary, oov_token=\"\")\n",
    "        self.num_to_char = tf.keras.layers.StringLookup(\n",
    "            vocabulary=self.vocabulary, oov_token=\"\", invert=True)\n",
    "\n",
    "    def _create_word_vocabulary(self):\n",
    "        words = set()\n",
    "        print(f\"Processing alignment files from: {self.alignment_path}\")\n",
    "        \n",
    "        for align_path in self.alignment_paths:\n",
    "            try:\n",
    "                with open(align_path, 'r') as f:\n",
    "                    content = f.read().strip().split()\n",
    "                    words.update([content[i] for i in range(2, len(content), 3)])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {align_path}: {str(e)}\")\n",
    "        \n",
    "        words.discard('sil')\n",
    "        vocabulary = sorted(list(words))\n",
    "        \n",
    "        if not vocabulary:\n",
    "            print(\"No words found in alignment files. Using default vocabulary.\")\n",
    "            vocabulary = ['bin', 'blue', 'at', 'f', 'two', 'now']\n",
    "        \n",
    "        print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "        return vocabulary\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.video_paths) // self.batch_size)\n",
    "    \n",
    "    def _process_video(self, video_path):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            mouth = gray[190:236, 80:220]\n",
    "            mouth = cv2.resize(mouth, (self.image_width, self.image_height))\n",
    "            frames.append(mouth)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        frames = np.array(frames, dtype=np.float32)\n",
    "        frames = (frames - frames.mean()) / (frames.std() + 1e-6)\n",
    "        \n",
    "        if len(frames) < self.frame_length:\n",
    "            pad_length = self.frame_length - len(frames)\n",
    "            frames = np.pad(frames, ((0, pad_length), (0, 0), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            frames = frames[:self.frame_length]\n",
    "        \n",
    "        return frames\n",
    "    \n",
    "    def _process_alignment(self, alignment_path):\n",
    "        with open(alignment_path, 'r') as f:\n",
    "            content = f.read().strip().split()\n",
    "        \n",
    "        words = [content[i] for i in range(2, len(content), 3) if content[i] != 'sil']\n",
    "        text = ' '.join(words)\n",
    "        return self.char_to_num(tf.convert_to_tensor(text.split()))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_videos = self.video_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_alignments = self.alignment_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        X = np.zeros((len(batch_videos), self.frame_length, self.image_height, self.image_width, 1))\n",
    "        Y = np.zeros((len(batch_videos), len(self.vocabulary)))\n",
    "        \n",
    "        for i, (video_path, align_path) in enumerate(zip(batch_videos, batch_alignments)):\n",
    "            frames = self._process_video(video_path)\n",
    "            X[i] = frames.reshape(self.frame_length, self.image_height, self.image_width, 1)\n",
    "            \n",
    "            labels = self._process_alignment(align_path)\n",
    "            Y[i] = tf.reduce_max(tf.one_hot(labels, len(self.vocabulary)), axis=0)\n",
    "        \n",
    "        return X, Y\n",
    "\n",
    "def build_model(frame_length, image_height, image_width, vocabulary_size):\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(frame_length, image_height, image_width, 1)), \n",
    "        Conv3D(64, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPool3D(pool_size=(1, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv3D(128, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPool3D(pool_size=(1, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Conv3D(256, kernel_size=(3, 3, 3), activation='relu'),\n",
    "        MaxPool3D(pool_size=(1, 2, 2)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        Reshape((-1, 256)),\n",
    "        \n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Bidirectional(LSTM(64)),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(vocabulary_size, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_save_main_model(data_dir, alignment_dir, batch_size=32):\n",
    "    print(\"Starting training for sentence-level prediction...\")\n",
    "    \n",
    "    model_dir = \"models_main\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    data_generator = LipReadingDataGenerator(data_dir, alignment_dir, batch_size=batch_size)\n",
    "    \n",
    "    model = build_model(\n",
    "        frame_length=75,\n",
    "        image_height=46,\n",
    "        image_width=140,\n",
    "        vocabulary_size=len(data_generator.vocabulary)\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(model_dir, 'lip_reading_main_best.keras'),\n",
    "            save_best_only=True,\n",
    "            monitor='accuracy'\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    model.fit(\n",
    "        data_generator,\n",
    "        epochs=10,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    final_model_path = os.path.join(model_dir, 'lip_reading_main_final.h5')\n",
    "    model.save(final_model_path)\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "\n",
    "    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        f.write('\\n'.join(data_generator.vocabulary))\n",
    "    print(f\"Vocabulary saved: {vocab_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = r\"/Users/sureshkumar/Desktop/Mouthmap/datas/s1\"\n",
    "    alignment_dir = r\"/Users/sureshkumar/Desktop/Mouthmap/datas/alignments/s1\"\n",
    "    \n",
    "    print(\"Training main model...\")\n",
    "    train_and_save_main_model(data_dir, alignment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 video files and 51 alignment files\n",
      "Processing alignment files from: /Users/sureshkumar/Desktop/Mouthmap/datas/validatation_alignments\n",
      "Vocabulary size: 31\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 28s/step - accuracy: 0.2188 - loss: 20.3880\n",
      "Validation Loss: 20.388019561767578, Validation Accuracy: 0.21875\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_dir, validation_data_dir):\n",
    "    model = load_model(os.path.join(model_dir, 'lip_reading_main_final.h5'))\n",
    "    validation_generator = LipReadingDataGenerator(validation_data_dir, val_alignment_dir, batch_size=32)\n",
    "    loss, accuracy = model.evaluate(validation_generator)\n",
    "    print(f\"Validation Loss: {loss}, Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_directory = \"/Users/sureshkumar/Desktop/Mouthmap/tf_envi/models_main\"\n",
    "    validation_data_dir = r\"/Users/sureshkumar/Desktop/Mouthmap/datas/validatation_data\"\n",
    "    val_alignment_dir=r\"/Users/sureshkumar/Desktop/Mouthmap/datas/validatation_alignments\"\n",
    "    # Update this path\n",
    "    evaluate_model(model_directory, validation_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions...\n",
      "Found 0 video files and 0 alignment files\n",
      "Processing alignment files from: \n",
      "No words found in alignment files. Using default vocabulary.\n",
      "Vocabulary size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Predicted sentence: in\n"
     ]
    }
   ],
   "source": [
    "def predict_with_main_model(model_dir, video_path):\n",
    "    vocab_path = os.path.join(model_dir, 'vocabulary_main.txt')\n",
    "    with open(vocab_path, 'r') as f:\n",
    "        vocabulary = f.read().splitlines()\n",
    "\n",
    "    data_generator = LipReadingDataGenerator(\"\", \"\")\n",
    "    data_generator.vocabulary = vocabulary\n",
    "    data_generator.char_to_num = tf.keras.layers.StringLookup(\n",
    "        vocabulary=vocabulary, oov_token=\"\")\n",
    "    data_generator.num_to_char = tf.keras.layers.StringLookup(\n",
    "        vocabulary=vocabulary, oov_token=\"\", invert=True)\n",
    "\n",
    "    model = load_model(os.path.join(model_dir, 'lip_reading_main_final.h5'))\n",
    "\n",
    "    frames = data_generator._process_video(video_path)\n",
    "    frames = frames.reshape(1, data_generator.frame_length, \n",
    "                            data_generator.image_height, \n",
    "                            data_generator.image_width, 1)\n",
    "\n",
    "    prediction = model.predict(frames)\n",
    "    predicted_indices = tf.argmax(prediction, axis=1).numpy()  # Convert to NumPy array\n",
    "\n",
    "    # Convert indices to characters\n",
    "    predicted_text = ' '.join([vocabulary[int(idx)] for idx in predicted_indices])  # Use vocabulary directly\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_directory = \"/Users/sureshkumar/Desktop/Mouthmap/tf_envi/models_main\"\n",
    "    test_video = r\"/Users/sureshkumar/Desktop/Mouthmap/Mouthmap/data/s1/bbbf6n.mpg\"\n",
    "\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    sentence_prediction = predict_with_main_model(model_directory, test_video)\n",
    "    print(f\"Predicted sentence: {sentence_prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_envi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
